{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font = {'size':'18', 'color':'black', 'verticalalignment':'bottom',  'fontstyle':'bold'} \n",
    "axis_font = { 'size':'14'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aut_df = pd.read_csv('/Users/codyotoole/Desktop/Research_Data/GeneDrive_Data/MetaData/Authors_10.csv')\n",
    "p_df = pd.read_csv('/Users/codyotoole/Desktop/Research_Data/GeneDrive_Data/MetaData/gd_full_meta_20002018_10auth.csv')\n",
    "ap_df = pd.read_csv('/Users/codyotoole/Desktop/Research_Data/GeneDrive_Data/MetaData/PaperAuthors_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn About the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Network contain %s papers from %s authors.'%(len(ap_df.PaperID.unique()), \n",
    "                                                     len(ap_df.AuthorID.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of publications per author\n",
    "plt.figure(figsize=(6,3))\n",
    "mpl.rcParams['axes.linewidth'] = 1 #set the value globally\n",
    "mpl.rcParams['axes.edgecolor'] = 'k'\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "\n",
    "aut_pap_cnt = dict(ap_df['AuthorID'].value_counts())\n",
    "\n",
    "xmin = min(aut_pap_cnt.values())\n",
    "xmax = max(aut_pap_cnt.values())\n",
    "bins = (xmax-xmin)\n",
    "plt.xlim(xmin-1, xmax+1)\n",
    "\n",
    "ax = plt.hist(list(aut_pap_cnt.values()), bins=bins, log=1, width=0.95, color='r', edgecolor='none', alpha=0.8 );\n",
    "# [i.set_linewidth(0.1) for i in ax.spines.itervalues()]\n",
    "\n",
    "plt.xlabel('# Papers per author', **axis_font)\n",
    "plt.ylabel('# Authors ', **axis_font)\n",
    "\n",
    "plt.grid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the kinds of papers each author tends to publish \n",
    "ptype_df = p_df[['PaperID', 'PaperType']] \n",
    "ptype_df = ptype_df.join(pd.get_dummies(ptype_df['PaperType']), how='outer')\n",
    "ap_df = ap_df.merge(ptype_df.rename(columns={'ID':'PaperID'}), on='PaperID').drop('ID', axis =1)\n",
    "nip_df = ap_df.copy()\n",
    "nip_df.drop('PaperType', axis=1, inplace=True)\n",
    "nip_df = nip_df.groupby('AuthorID').sum().reset_index()\n",
    "nip_df = nip_df.merge(aut_df.rename(columns={'Id':'AuthorID'}), on='AuthorID')\n",
    "auth = nip_df['Author']\n",
    "nip_df.drop(labels=['Author'], axis=1,inplace = True)\n",
    "nip_df.insert(0, 'Author', auth)\n",
    "\n",
    "nip_df['Total_Publications'] = nip_df['GE research']+nip_df['ethics/policy']+nip_df['gene research']+nip_df['insect research']+nip_df['population dynamics']+nip_df['review']+nip_df['wolbachia']          \n",
    "\n",
    "for i in 'GE research','ethics/policy', 'gene research', 'insect research', 'population dynamics', 'review', 'wolbachia':\n",
    "    nip_df[i+'_ratio'] = nip_df[i]/nip_df['Total_Publications']\n",
    "\n",
    "nip_df.sort_values(by=['Total_Publications'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author-author interaction network\n",
    "#first make a bipartite graph of author-paper\n",
    "''' First need to map both author and paper ids to start\n",
    "    from zero so that we can store them in a numpy array. '''\n",
    "\n",
    "int_pid = dict(enumerate(list(ap_df.PaperID.unique())))\n",
    "int_aid = dict(enumerate(list(ap_df.AuthorID.unique())))\n",
    "\n",
    "#make index of papers and authors\n",
    "pid_intid = {v:k for k,v in int_pid.items()}\n",
    "aid_intid = {v:k for k,v in int_aid.items()}\n",
    "\n",
    "\n",
    "ap_tuples = list(zip(ap_df.AuthorID, ap_df.PaperID))\n",
    "ap_int_tups =  [(aid_intid[i[0]], pid_intid[i[1]]) for i in ap_tuples]\n",
    "\n",
    "''' AP: matrix of author-paper, AP[i, j]=1 indicates that author i has published paper j '''\n",
    "AP = sp.csc_matrix((np.ones(len(ap_int_tups)), zip(*ap_int_tups)))\n",
    "\n",
    "\n",
    "''' AA: the author-author matrix, \n",
    "    AA[i, j]=1 indicates that author i has published a paper with author j '''\n",
    "AA = AP.dot(AP.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting numpy 2D array to network\n",
    "'Remove self-loops'\n",
    "AA = np.array(AA - np.diag(AA.diagonal()))\n",
    "\n",
    "'Weighted graph'\n",
    "G = nx.from_numpy_matrix(AA, parallel_edges=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute network centrality measures to add to dataframe\n",
    "'''number of co-authors per member '''\n",
    "node_deg = nx.degree(G) \n",
    "\n",
    "'''normalized number of co-authors '''\n",
    "deg_cent = nx.degree_centrality(G) \n",
    "\n",
    "'''fraction of the number of times the author appears on the path connecting two other authors '''\n",
    "bet_cent = nx.betweenness_centrality(G)\n",
    "\n",
    "close_cent = nx.closeness_centrality(G)\n",
    "\n",
    "eigen_cent = nx.eigenvector_centrality(G)\n",
    "\n",
    "nip_df['Degree'] = nip_df['AuthorID'].apply(lambda l: node_deg[aid_intid.get(l)])\n",
    "nip_df['Deg_Cent'] = nip_df['AuthorID'].apply(lambda l: deg_cent[aid_intid.get(l)])\n",
    "nip_df['Betweenness'] = nip_df['AuthorID'].apply(lambda l: bet_cent.get(aid_intid.get(l)))\n",
    "nip_df['Closeness'] = nip_df['AuthorID'].apply(lambda l: close_cent.get(aid_intid.get(l)))\n",
    "nip_df['Eigenvector'] = nip_df['AuthorID'].apply(lambda l: eigen_cent.get(aid_intid.get(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_edges(), G.number_of_nodes(), nx.density(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = G.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid_name = dict(zip(aut_df.Id, aut_df.Author))\n",
    "\n",
    "for node in K.nodes():\n",
    "    name = aid_name.get(int_aid[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nid_intid = {}\n",
    "    \n",
    "for j,c in aid_intid.items():\n",
    "    for k,v in aid_name.items():\n",
    "        if k == j:\n",
    "            nid_intid[c] = v\n",
    "            \n",
    "K=nx.relabel_nodes(K,nid_intid)\n",
    "#Relabels nodes with real author names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In my network I want to focus on the giant component\n",
    "#So I need to remove the isolates\n",
    "\n",
    "nip_sub = nip_df[(nip_df['GE research']+nip_df['ethics/policy']+nip_df['gene research']+nip_df['insect research']+nip_df['population dynamics']+nip_df['review']+nip_df['wolbachia'])>1]\n",
    "sub_authors =nip_sub['Author'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = K.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in list(nx.connected_components(H)):\n",
    "    if any(x in sub_authors for x in list(component)):\n",
    "        continue\n",
    "    else:\n",
    "            for node in component:\n",
    "                        H.remove_node(node)\n",
    " \n",
    "len(list(H.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Gephi for Visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(H, 'gene_drive.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "Average shortest path, degree correlation, degree distribution, rich club, preferential attachment, strength of ties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Path Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_shortest_path_length(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_pearson_correlation_coefficient(H)\n",
    "#Social Networks are often assortative due to formation of communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = G.degree()\n",
    "degree_values = dict(degrees).values()\n",
    "degree_values = sorted(degree_values)\n",
    "histogram = [degree_values.count(i)/float(nx.number_of_nodes(G)) \\\n",
    "for i in degree_values]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(degree_values,histogram, 'o', color='r', markersize = 5)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('p(k)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferential Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = G.degree()\n",
    "degree_values = dict(degrees).values()\n",
    "degree_values = sorted(degree_values)\n",
    "k = np.asarray(degree_values)\n",
    "j = np.asarray(degree_values)\n",
    "\n",
    "pa = []\n",
    "for n in k:\n",
    "    p = n/(np.sum(j))\n",
    "    pa.append((n, p))\n",
    " \n",
    "list1, list2 = zip(*pa) \n",
    "\n",
    "\n",
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "    return unique_list\n",
    "\n",
    "list2 = unique(list2)\n",
    "\n",
    "pa3 = []\n",
    "for n in range(0, len(list2)):\n",
    "    p = np.sum(list2[0:n+1])\n",
    "    pa3.append(p)\n",
    "            \n",
    "deg = list(set(degree_values))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(deg, pa3, 'o', mfc='none', color='r', markersize=10, markeredgewidth=2.2)\n",
    "#plt.plot(deg, [number ** 2 for number in deg], '-')\n",
    "#plt.plot(deg, deg, '-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('p(k)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0.8, 10**2)\n",
    "plt.ylim(10**-4.2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rich-club Structure (The rich get richer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.utils import accumulate\n",
    "from networkx.utils import not_implemented_for\n",
    "\n",
    "def _compute_rc(G):\n",
    "    \"\"\"Returns the rich-club coefficient for each degree in the graph\n",
    "    `G`.\n",
    "\n",
    "    `G` is an undirected graph without multiedges.\n",
    "\n",
    "    Returns a dictionary mapping degree to rich-club coefficient for\n",
    "    that degree.\n",
    "\n",
    "    \"\"\"\n",
    "    deghist = nx.degree_histogram(G)\n",
    "    total = sum(deghist)\n",
    "    # Compute the number of nodes with degree greater than `k`, for each\n",
    "    # degree `k` (omitting the last entry, which is zero).\n",
    "    nks = (total - cs for cs in accumulate(deghist) if total - cs > 1)\n",
    "    # Create a sorted list of pairs of edge endpoint degrees.\n",
    "    #\n",
    "    # The list is sorted in reverse order so that we can pop from the\n",
    "    # right side of the list later, instead of popping from the left\n",
    "    # side of the list, which would have a linear time cost.\n",
    "    edge_degrees = sorted((sorted(map(G.degree,  e)) for e in G.edges()),\n",
    "                          reverse=True)\n",
    "    ek = G.number_of_edges()\n",
    "    k1, k2 = edge_degrees.pop()\n",
    "    rc = {}\n",
    "    for d, nk in enumerate(nks):\n",
    "        while k1 <= d:\n",
    "            if len(edge_degrees) == 0:\n",
    "                ek = 0\n",
    "                break\n",
    "            k1, k2 = edge_degrees.pop()\n",
    "            ek -= 1\n",
    "        rc[d] = 2 * ek / (nk * (nk - 1))\n",
    "    return rc\n",
    "\n",
    "\n",
    "# make R a copy of G, randomize with Q*|E| double edge swaps\n",
    "# and use rich_club coefficient of R to normalize\n",
    "rr = {}\n",
    "rc = _compute_rc(G)\n",
    "R = G.copy()\n",
    "E = R.number_of_edges()\n",
    "nx.double_edge_swap(R, 100 * E, max_tries=100 * E * 10, seed=None)\n",
    "rcran = _compute_rc(R)\n",
    "\n",
    "for k, v in rc.items():\n",
    "                rr[k] = v / rcran[k]\n",
    "            \n",
    "\n",
    "keys = [] \n",
    "values = [] \n",
    "items = rr.items() \n",
    "for item in items: \n",
    "    keys.append(item[0]), values.append(item[1])\n",
    "\n",
    "\n",
    "plt.plot(keys, values, 'o', mfc='none', color='r', markersize=10, markeredgewidth=2.2)\n",
    "#plt.plot(deg, [number ** 2 for number in deg], '-')\n",
    "plt.axhline(y=1, color='black', linestyle='-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('p(k)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strength of Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = []\n",
    "for u,v,data in H.edges(data=True):\n",
    "    weight.append((u, v, data['weight']))\n",
    "    \n",
    "weight_df = pd.DataFrame(weight, columns=[\"Author1\", \"Author2\", \"Edge_Weight\"])\n",
    "    \n",
    "qqq = []\n",
    "for n in range(0, len(weight_df)):\n",
    "    if weight_df['Edge_Weight'][n] > 2:\n",
    "        qqq.append(weight_df['Edge_Weight'][n])\n",
    "\n",
    "print(len(qqq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Visualization (Community Detection through Node2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate walks\n",
    "node2vec = Node2Vec(H, dimensions=10, walk_length=10, num_walks=100)\n",
    "\n",
    "# Learn embeddings \n",
    "model = node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get vectors into numpy array\n",
    "X3 = np.array(model.wv.vectors, dtype='float') \n",
    "\n",
    "node_sub = list(H.nodes)\n",
    "\n",
    "X3_vectors = []\n",
    "\n",
    "for node in node_sub:\n",
    "    X3_vectors.append(model[node])\n",
    "    \n",
    "X3_vectors = np.array(X3_vectors)\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X3_reduced= pca.fit_transform(X3_vectors)\n",
    "X3_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kmeans with many different k\n",
    "\n",
    "distortions = []\n",
    "K = range(2, 30)\n",
    "for k in K:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X3_reduced)\n",
    "    k_means.fit(X3_reduced)\n",
    "    #x1.append(k)\n",
    "    distortions.append(sum(np.min(cdist(X3_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X3_vectors.shape[0])\n",
    "    #print('Found distortion for {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method\n",
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify colors and clusters you like\n",
    "n_cluster = 6\n",
    "kmeans = KMeans(n_clusters=n_cluster, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X3_reduced)\n",
    "\n",
    "kmeans_df = pd.DataFrame({'Author':node_sub})\n",
    "kmeans_df['cluster'] = kmeans.labels_\n",
    "kmeans_df['color'] = kmeans_df['cluster']\n",
    "\n",
    "\n",
    "for n in range(0, len(kmeans_df['cluster'])):\n",
    "    if kmeans_df['cluster'][n] == 0:\n",
    "        kmeans_df['color'][n] = 'green'\n",
    "    if kmeans_df['cluster'][n] == 1:\n",
    "        kmeans_df['color'][n] = 'blue'\n",
    "    if kmeans_df['cluster'][n] == 2:\n",
    "        kmeans_df['color'][n] = 'yellow'\n",
    "    if kmeans_df['cluster'][n] == 3:\n",
    "        kmeans_df['color'][n] = 'red'\n",
    "    if kmeans_df['cluster'][n] == 4:\n",
    "        kmeans_df['color'][n] = 'purple'\n",
    "    if kmeans_df['cluster'][n] == 5:\n",
    "        kmeans_df['color'][n] = 'orange'\n",
    "    if kmeans_df['cluster'][n] == 6:\n",
    "        kmeans_df['color'][n] = 'violet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, len(kmeans_df)):  \n",
    "    H.nodes[kmeans_df['Author'][n]]['color'] = kmeans_df['color'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export for gephi viz again\n",
    "nx.write_gexf(H, 'node2vec_gd.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
